<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Building the PE Org-AI-R Platform</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      font-size: 11pt;
      line-height: 1.6;
      max-width: 900px;
      margin: 40px auto;
      color: #202124;
    }

    /* ── Headings ── */
    h1 {
      font-size: 26pt;
      font-weight: 700;
      margin-bottom: 6px;
    }
    h2 {
      font-size: 16pt;
      font-weight: 700;
      margin-top: 36px;
      margin-bottom: 8px;
      border-bottom: 2px solid #4285f4;
      padding-bottom: 4px;
      color: #1a73e8;
    }
    h3 {
      font-size: 13pt;
      font-weight: 700;
      margin-top: 20px;
      margin-bottom: 6px;
    }

    /* ── Metadata block ── */
    .metadata {
      border: 1px solid #dadce0;
      border-radius: 4px;
      padding: 12px 16px;
      margin: 16px 0 24px;
      font-size: 10pt;
      background: #f8f9fa;
    }
    .metadata p { margin: 4px 0; }
    .metadata strong { color: #202124; }

    /* ── Duration badge ── */
    .duration {
      display: inline-block;
      background: #e8f0fe;
      color: #1a73e8;
      border-radius: 4px;
      padding: 2px 8px;
      font-size: 10pt;
      font-weight: 600;
      margin-bottom: 10px;
    }

    /* ── Note / callout ── */
    .note {
      background: #fef7e0;
      border-left: 4px solid #fbbc04;
      padding: 10px 14px;
      margin: 12px 0;
      font-size: 10pt;
      border-radius: 0 4px 4px 0;
    }

    /* ── Code blocks ── */
    pre {
      font-family: "Courier New", Courier, monospace;
      font-size: 10pt;
      background: #f1f3f4;
      border: 1px solid #dadce0;
      border-radius: 4px;
      padding: 14px 16px;
      overflow-x: auto;
      white-space: pre;
      margin: 10px 0 16px;
    }
    code {
      font-family: "Courier New", Courier, monospace;
      font-size: 10pt;
      background: #f1f3f4;
      padding: 1px 4px;
      border-radius: 3px;
    }
    pre code {
      background: none;
      padding: 0;
    }

    /* ── Tables ── */
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 12px 0 18px;
      font-size: 10pt;
    }
    th {
      background: #4285f4;
      color: #fff;
      text-align: left;
      padding: 8px 10px;
      font-weight: 600;
    }
    td {
      padding: 7px 10px;
      border: 1px solid #dadce0;
    }
    tr:nth-child(even) td {
      background: #f8f9fa;
    }

    /* ── Lists ── */
    ul, ol { padding-left: 22px; margin: 8px 0; }
    li { margin-bottom: 4px; }

    /* ── Separator ── */
    hr {
      border: none;
      border-top: 1px solid #dadce0;
      margin: 32px 0;
    }

    /* ── Congratulations checkmarks ── */
    .checklist li { list-style: none; padding-left: 4px; }
    .checklist li::before { content: "✓  "; color: #34a853; font-weight: 700; }
  </style>
</head>
<body>

<!-- ═══════════════════════════════════════════════════════════
     TITLE  (select this line → apply "Title" style in Google Docs)
     ═══════════════════════════════════════════════════════════ -->
<h1>Building the PE Org-AI-R Platform: AI Readiness Scoring for Private Equity</h1>

<div class="metadata">
  <p><strong>Author:</strong> Wei Cheng Tu, Nisarg Sheth, Yu Tzu Li</p>
  <p><strong>Summary:</strong> Step-by-step guide to setting up and running the PE Org-AI-R Platform — a FastAPI + Snowflake + Redis system that computes AI-readiness scores for private equity portfolio companies using SEC filings, external signals, and the Org-AI-R scoring formula.</p>
  <p><strong>Category:</strong> Data Engineering, AI/ML, FinTech</p>
  <p><strong>Environment:</strong> web</p>
  <p><strong>Status:</strong> Draft</p>
  <p><strong>Feedback Link:</strong> https://github.com/DAMG7245-spring2026/case_study3/issues</p>
</div>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>1. Overview</h2>

<p>In this codelab you will build and run the PE Org-AI-R Platform — an end-to-end AI-readiness assessment system for private equity portfolio companies.</p>

<h3>What you'll learn</h3>
<ul>
  <li>How to ingest SEC filings (10-K, 10-Q, 8-K) via SEC EDGAR and chunk them into section-aware segments</li>
  <li>How to collect four categories of external operational signals (jobs, tech stack, patents, leadership)</li>
  <li>How the Org-AI-R formula combines seven AI-readiness dimensions into a single composite score</li>
  <li>How to operate a FastAPI backend, Redis cache, Snowflake data warehouse, Streamlit dashboard, and Airflow DAG together</li>
</ul>

<h3>What you'll build</h3>
<p>A running instance of the platform with:</p>
<ul>
  <li>REST API (FastAPI) at <code>http://localhost:8000</code></li>
  <li>Scoring dashboard (Streamlit) at <code>http://localhost:8501</code></li>
  <li>Pipeline orchestration (Airflow) at <code>http://localhost:8080</code></li>
</ul>

<p><strong>Target companies:</strong> ADP, CAT, DE, GS, HCA, JPM, PAYX, TGT, UNH, WMT</p>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>2. Prerequisites</h2>
<div class="duration">Duration: 5 minutes</div>

<p>Before starting, make sure you have the following installed and accounts created.</p>

<h3>Software</h3>
<ul>
  <li>Python 3.11+</li>
  <li>Poetry 1.5+ — dependency manager</li>
  <li>Docker Desktop — for Redis, Airflow</li>
  <li>Git</li>
</ul>

<h3>Accounts &amp; API Keys (Required)</h3>
<ul>
  <li><strong>Snowflake</strong> — Free trial at https://signup.snowflake.com/</li>
  <li><strong>AWS S3 bucket</strong> (optional — for raw filing archival)</li>
</ul>

<h3>External API Keys (Optional — graceful degradation if missing)</h3>
<table>
  <tr><th>Key</th><th>Service</th><th>Signal Category</th></tr>
  <tr><td><code>SERPAPI_KEY</code></td><td>SerpApi</td><td>Job postings</td></tr>
  <tr><td><code>BUILTWITH_API_KEY</code></td><td>BuiltWith</td><td>Tech stack</td></tr>
  <tr><td><code>LENS_API_KEY</code></td><td>Lens.org</td><td>Patents</td></tr>
  <tr><td><code>LINKEDIN_API_KEY</code></td><td>LinkedIn</td><td>Leadership</td></tr>
</table>

<div class="note"><strong>Note:</strong> The pipeline runs with graceful degradation. Missing API keys result in empty signal collections, not errors.</div>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>3. Clone the Repository and Install Dependencies</h2>
<div class="duration">Duration: 5 minutes</div>

<h3>Clone and install</h3>
<pre><code># Clone the repository
git clone https://github.com/DAMG7245-spring2026/case_study3.git
cd case_study3

# Install Poetry (if not installed)
curl -sSL https://install.python-poetry.org | python3 -

# Install all project dependencies
poetry install</code></pre>

<h3>Verify installation</h3>
<pre><code>poetry run python -c "import fastapi; print('FastAPI OK')"
poetry run python -c "import snowflake.connector; print('Snowflake OK')"</code></pre>

<p>Expected output:</p>
<pre><code>FastAPI OK
Snowflake OK</code></pre>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>4. Configure the Environment</h2>
<div class="duration">Duration: 10 minutes</div>

<h3>Create the .env file</h3>
<pre><code>cp .env.example .env   # if template exists, otherwise create manually</code></pre>

<p>Open <code>.env</code> and set the following values:</p>
<pre><code># ── Application ──────────────────────────────────────────────
APP_NAME="PE Org-AI-R Platform"
APP_VERSION="1.0.0"
DEBUG=true

# ── Snowflake (REQUIRED) ─────────────────────────────────────
SNOWFLAKE_ACCOUNT=xy12345.us-east-1
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_DATABASE=PE_ORG_AIR
SNOWFLAKE_SCHEMA=PUBLIC
SNOWFLAKE_WAREHOUSE=COMPUTE_WH

# ── Redis (REQUIRED) ─────────────────────────────────────────
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# ── AWS S3 (OPTIONAL) ────────────────────────────────────────
AWS_ACCESS_KEY_ID=your_access_key_id
AWS_SECRET_ACCESS_KEY=your_secret_access_key
AWS_REGION=us-east-1
S3_BUCKET=your-bucket-name

# ── External API Keys (OPTIONAL) ─────────────────────────────
SERPAPI_KEY=your_serpapi_key
BUILTWITH_API_KEY=your_builtwith_key
LENS_API_KEY=your_lens_token
LINKEDIN_API_KEY=your_linkedin_key</code></pre>

<h3>Snowflake setup</h3>
<p>Log in to https://app.snowflake.com and run:</p>
<pre><code>-- 1. Create warehouse
CREATE WAREHOUSE IF NOT EXISTS COMPUTE_WH WITH WAREHOUSE_SIZE = 'XSMALL';
USE WAREHOUSE COMPUTE_WH;

-- 2. Create database and schema
CREATE DATABASE IF NOT EXISTS PE_ORG_AIR;
USE DATABASE PE_ORG_AIR;
USE SCHEMA PUBLIC;

-- 3. Verify
SHOW DATABASES;
SHOW WAREHOUSES;</code></pre>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>5. Set Up the Database</h2>
<div class="duration">Duration: 10 minutes</div>

<h3>Start Redis</h3>
<pre><code># Option A: standalone Docker container
docker run -d --name redis-local -p 6379:6379 redis:7-alpine

# Verify Redis is running
docker ps | grep redis</code></pre>

<h3>Run Alembic migrations</h3>
<p>The migrations create all necessary tables in Snowflake.</p>
<pre><code>poetry run alembic upgrade head

# Verify current migration
poetry run alembic current
# Expected: 013_signal_dimension_weights (head)</code></pre>

<h3>Seed industry data</h3>
<p>Open the Snowflake Web UI and run:</p>
<pre><code>INSERT INTO industries (id, name, sector, h_r_base) VALUES
  ('550e8400-e29b-41d4-a716-446655440001', 'Manufacturing',       'Industrials', 72),
  ('550e8400-e29b-41d4-a716-446655440002', 'Healthcare Services', 'Healthcare',  78),
  ('550e8400-e29b-41d4-a716-446655440003', 'Business Services',   'Services',    75),
  ('550e8400-e29b-41d4-a716-446655440004', 'Retail',              'Consumer',    70),
  ('550e8400-e29b-41d4-a716-446655440005', 'Financial Services',  'Financial',   80);

SELECT * FROM industries;</code></pre>

<h3>Seed portfolio companies</h3>
<pre><code>poetry run python scripts/seed_target_companies.py</code></pre>

<h3>Database schema overview</h3>
<table>
  <tr><th>Table</th><th>Purpose</th></tr>
  <tr><td><code>industries</code></td><td>Sector baselines and H^R starting values</td></tr>
  <tr><td><code>companies</code></td><td>Portfolio company information</td></tr>
  <tr><td><code>assessments</code></td><td>AI readiness assessment records</td></tr>
  <tr><td><code>dimension_scores</code></td><td>Per-dimension scores (7 dimensions)</td></tr>
  <tr><td><code>documents</code></td><td>SEC filing metadata (10-K, 10-Q, 8-K)</td></tr>
  <tr><td><code>document_chunks</code></td><td>Section-aware text segments (~500 words)</td></tr>
  <tr><td><code>external_signals</code></td><td>Raw operational signals with normalized scores</td></tr>
  <tr><td><code>company_signal_summaries</code></td><td>Aggregated composite signal scores</td></tr>
</table>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>6. Start the Platform</h2>
<div class="duration">Duration: 5 minutes</div>

<h3>Option A: Start services individually</h3>
<pre><code># Terminal 1 — FastAPI backend
poetry run uvicorn app.main:app --reload

# Terminal 2 — Streamlit dashboard
poetry run streamlit run streamlit_ui/main.py</code></pre>

<h3>Option B: Docker Compose (all services)</h3>
<pre><code>cd docker
docker-compose --env-file ../.env up -d

# View logs
docker-compose logs -f api</code></pre>

<h3>Verify the API is running</h3>
<pre><code>curl http://localhost:8000/health
# Expected: {"status": "healthy", "version": "1.0.0"}</code></pre>

<p>You can now open:</p>
<table>
  <tr><th>Service</th><th>URL</th></tr>
  <tr><td>Swagger UI</td><td>http://localhost:8000/docs</td></tr>
  <tr><td>ReDoc</td><td>http://localhost:8000/redoc</td></tr>
  <tr><td>Streamlit dashboard</td><td>http://localhost:8501</td></tr>
  <tr><td>Airflow</td><td>http://localhost:8080 (admin / admin)</td></tr>
</table>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>7. Collect Evidence</h2>
<div class="duration">Duration: 15 minutes</div>

<p>Evidence collection has two parts: SEC filings and external signals.</p>

<h3>Trigger via API</h3>
<pre><code># Collect both SEC documents and external signals for three companies
curl -X POST "http://localhost:8000/api/v1/evidence/backfill" \
  -H "Content-Type: application/json" \
  -d '{
    "tickers": ["JPM", "WMT", "GS"],
    "include_documents": true,
    "include_signals": true,
    "years_back": 3
  }'

# Example response:
# {
#   "task_id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
#   "status": "queued",
#   "companies_queued": 3,
#   "message": "Backfill started for 3 companies: JPM, WMT, GS"
# }

# Monitor progress
curl http://localhost:8000/api/v1/evidence/stats</code></pre>

<h3>Trigger via CLI script</h3>
<pre><code># All 10 companies — documents + signals
poetry run python scripts/collect_evidence.py --companies all

# Documents only (faster, no API keys needed)
poetry run python scripts/collect_evidence.py \
  --companies JPM,WMT,GS \
  --documents-only

# Signals only
poetry run python scripts/collect_evidence.py \
  --companies JPM,WMT \
  --signals-only</code></pre>

<h3>Four external signal categories</h3>
<table>
  <tr><th>Category</th><th>Source</th><th>Weight</th></tr>
  <tr><td><code>technology_hiring</code></td><td>SerpApi job ads</td><td>30%</td></tr>
  <tr><td><code>digital_presence</code></td><td>BuiltWith</td><td>25%</td></tr>
  <tr><td><code>innovation_activity</code></td><td>Lens.org patents</td><td>25%</td></tr>
  <tr><td><code>leadership_signals</code></td><td>Website / LinkedIn</td><td>20%</td></tr>
</table>

<h3>New signals in Case Study 3</h3>
<pre><code># Load Glassdoor culture/talent data from a local file
poetry run python scripts/load_glassdoor_from_file.py \
  --file data/glassdoor_export.json</code></pre>

<p>The Board Composition Analyzer automatically pulls AI-governance signals from company proxy filings via the SEC EDGAR pipeline and stores them as dimension scores for the <code>ai_governance</code> dimension.</p>

<h3>Validate collected evidence in Snowflake</h3>
<pre><code>-- Document count by company and filing type
SELECT ticker, filing_type, COUNT(*) AS docs, SUM(chunk_count) AS chunks
FROM documents
WHERE ticker IN ('JPM','WMT','GS')
GROUP BY ticker, filing_type
ORDER BY ticker, filing_type;

-- Composite signal score per company
SELECT c.ticker, css.composite_score, css.signal_count
FROM companies c
JOIN company_signal_summaries css ON c.id = css.company_id
ORDER BY css.composite_score DESC;</code></pre>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>8. Understand the Org-AI-R Scoring Formula</h2>
<div class="duration">Duration: 10 minutes</div>

<h3>Seven AI-Readiness Dimensions</h3>
<p>All scoring starts from these seven dimensions, each scored 0–100.</p>
<table>
  <tr><th>Dimension</th><th>Key Evidence Sources</th></tr>
  <tr><td>Data Infrastructure</td><td>SEC filings Item 7, tech signals</td></tr>
  <tr><td>AI Governance</td><td>Board composition, proxy filings</td></tr>
  <tr><td>Technology Stack</td><td>BuiltWith, job postings</td></tr>
  <tr><td>Talent &amp; Skills</td><td>Job postings (AI/ML roles), Glassdoor</td></tr>
  <tr><td>Leadership Vision</td><td>SEC Item 1, leadership signals</td></tr>
  <tr><td>Use Case Portfolio</td><td>SEC 8-K, patent filings</td></tr>
  <tr><td>Culture &amp; Change</td><td>Glassdoor reviews, Glassdoor ratings</td></tr>
</table>

<h3>The Org-AI-R formula</h3>
<pre><code>Org-AI-R = (1 − β) × [α × V^R + (1 − α) × H^R] + β × Synergy

Where:
  α    = 0.60   (V^R weight vs H^R)
  β    = 0.12   (Synergy component weight)

  V^R      = WeightedMean(7 dimensions) × PenaltyFactor × TalentRiskAdj
  H^R      = H^R_base × (1 + 0.15 × PositionFactor)
  Synergy  = (V^R × H^R / 100) × Alignment × TimingFactor
  CI       = score ± z × SEM   (Spearman-Brown reliability, 95%)</code></pre>

<h3>Sub-score definitions</h3>
<table>
  <tr><th>Sub-score</th><th>Meaning</th></tr>
  <tr><td><strong>V^R</strong> (Idiosyncratic)</td><td>Company-specific AI readiness based on internal evidence</td></tr>
  <tr><td><strong>H^R</strong> (Systematic)</td><td>Sector-level opportunity baseline adjusted by market position</td></tr>
  <tr><td><strong>Synergy</strong></td><td>Multiplicative benefit when V^R and H^R are both high</td></tr>
  <tr><td><strong>TC</strong> (Talent Concentration)</td><td>Key-person / talent-concentration risk (lowers V^R)</td></tr>
  <tr><td><strong>PF</strong> (Position Factor)</td><td>Company rank vs. sector peers by V^R and market cap</td></tr>
  <tr><td><strong>CI</strong> (Confidence Interval)</td><td>95% interval driven by evidence count via SEM</td></tr>
</table>

<h3>Sector H^R baselines</h3>
<table>
  <tr><th>Sector</th><th>H^R Base</th></tr>
  <tr><td>Financial</td><td>80</td></tr>
  <tr><td>Healthcare</td><td>78</td></tr>
  <tr><td>Business Services</td><td>75</td></tr>
  <tr><td>Manufacturing</td><td>72</td></tr>
  <tr><td>Retail</td><td>70</td></tr>
</table>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>9. Compute Org-AI-R Scores</h2>
<div class="duration">Duration: 10 minutes</div>

<h3>Option A: Score a single company via the API</h3>
<pre><code># Score by company UUID
curl -X POST \
  "http://localhost:8000/api/v1/scores/{company_id}/compute" \
  -H "Content-Type: application/json"

# Or score by ticker
curl -X POST \
  "http://localhost:8000/api/v1/scores/by-ticker/JPM" \
  -H "Content-Type: application/json"

# Example response:
# {
#   "ticker": "JPM",
#   "org_air_score": 74.35,
#   "vr_score": 72.10,
#   "hr_score": 81.20,
#   "synergy_score": 58.44,
#   "confidence_lower": 69.82,
#   "confidence_upper": 78.88,
#   "dimension_scores": {
#     "data_infrastructure": 78.5,
#     "ai_governance": 71.0,
#     "technology_stack": 76.2,
#     "talent_skills": 68.9,
#     "leadership_vision": 74.3,
#     "use_case_portfolio": 65.1,
#     "culture_change": 69.8
#   }
# }</code></pre>

<h3>Option B: Batch score all companies via CLI</h3>
<pre><code>poetry run python scripts/compute_scores.py

# Output:
# Scoring JPM ... done  (74.35)
# Scoring WMT ... done  (66.12)
# ...
# Portfolio mean Org-AI-R: 71.4</code></pre>

<h3>Option C: Interactive scoring calculator (Streamlit)</h3>
<p>Open <code>http://localhost:8501</code>, navigate to page <strong>"10 Scoring Calculator"</strong>, and manually adjust dimension sliders to explore the formula live.</p>

<h3>How the pipeline runs internally</h3>
<pre><code>1. Load company + industry from Snowflake
2. Load dimension_scores (already computed by dimension_scorer)
3. Analyze job postings → TalentConcentration (TC)
4. V^R     = VRCalculator(dimension_scores, TC)
5. PF      = PositionFactorCalculator(V^R, sector, market_cap_percentile)
6. H^R     = HRCalculator(sector, PF, H^R_base)
7. Synergy = SynergyCalculator(V^R, H^R, alignment, timing_factor=1.05)
8. Org-AI-R = OrgAIRCalculator(V^R, H^R, Synergy)
9. Attach 95% CI using Spearman-Brown SEM
10. Return OrgAIRScores dataclass</code></pre>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>10. Explore the Streamlit Dashboard</h2>
<div class="duration">Duration: 5 minutes</div>

<p>Navigate to <code>http://localhost:8501</code>. The dashboard has the following pages:</p>
<table>
  <tr><th>Page</th><th>What it shows</th></tr>
  <tr><td>0 Companies</td><td>Portfolio company list and metadata</td></tr>
  <tr><td>1 Dashboard</td><td>Top-level portfolio health summary</td></tr>
  <tr><td>2 Documents</td><td>SEC filings index per company</td></tr>
  <tr><td>3 Signals</td><td>External signal breakdown (4 categories)</td></tr>
  <tr><td>4 Evidence</td><td>Evidence coverage and source stats</td></tr>
  <tr><td>5 Scoring Dashboard</td><td>Portfolio-wide Org-AI-R overview with rankings</td></tr>
  <tr><td>6 Scoring Evidence</td><td>Evidence sources and coverage heatmap per company</td></tr>
  <tr><td>7 Scoring Dimensions</td><td>Radar chart of 7 dimensions per company</td></tr>
  <tr><td>8 Scoring Portfolio</td><td>Side-by-side company comparison</td></tr>
  <tr><td>9 Scoring Audit</td><td>Full audit trail: parameters, CI, all sub-scores</td></tr>
  <tr><td>10 Scoring Calculator</td><td>Interactive slider-based formula explorer</td></tr>
</table>

<h3>Example: Viewing JPM's dimension radar chart</h3>
<ol>
  <li>Open the Streamlit app at <code>http://localhost:8501</code></li>
  <li>Navigate to <strong>"7 Scoring Dimensions"</strong></li>
  <li>Select <strong>"JPM"</strong> from the company dropdown</li>
  <li>The radar chart shows all seven dimensions with scores overlaid on the sector average</li>
</ol>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>11. Airflow Orchestration</h2>
<div class="duration">Duration: 10 minutes</div>

<p>The Airflow DAG automates the full data pipeline on a schedule.</p>

<h3>Start Airflow (Docker Compose)</h3>
<pre><code>cd docker
docker-compose --env-file ../.env up -d airflow-db airflow-init
# Wait for init to complete, then:
docker-compose --env-file ../.env up -d airflow-webserver airflow-scheduler

# Open Airflow UI
open http://localhost:8080
# Login: admin / admin</code></pre>

<h3>DAG: org_air_pipeline</h3>
<p>The DAG <code>org_air_pipeline_dag.py</code> runs these tasks in sequence:</p>
<pre><code>Task 1: collect_documents
  └─ POST /api/v1/evidence/backfill
     (include_documents=true, include_signals=false)

Task 2: collect_signals
  └─ POST /api/v1/evidence/backfill
     (include_documents=false, include_signals=true)
     Includes: job_signals, patent_signals, glassdoor_collector, board_analyzer

Task 3: compute_dimension_scores
  └─ Runs dimension_scorer for each company
     Maps evidence → 7 AI-readiness dimensions via evidence_mapper

Task 4: compute_org_air_scores
  └─ Runs org_air_pipeline for each company
     Persists V^R, H^R, Synergy, Org-AI-R to Snowflake</code></pre>

<h3>Trigger the DAG manually</h3>
<pre><code># Via CLI inside the container
docker exec -it docker-airflow-scheduler-1 \
  airflow dags trigger org_air_pipeline</code></pre>

<p>Or use the Airflow UI:</p>
<ol>
  <li>Open <code>http://localhost:8080</code></li>
  <li>Find <strong>"org_air_pipeline"</strong></li>
  <li>Click the <strong>▶ Trigger DAG</strong> button</li>
</ol>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>12. Run the Tests</h2>
<div class="duration">Duration: 5 minutes</div>

<p>Always run tests before marking any change as complete.</p>
<pre><code># Run the full test suite
poetry run pytest

# Run with coverage report
poetry run pytest --cov=app --cov-report=term-missing

# Run a specific test file
poetry run pytest tests/test_scoring_properties.py -v

# Run tests matching a keyword
poetry run pytest -k "org_air" -v</code></pre>

<h3>Key test files</h3>
<table>
  <tr><th>Test File</th><th>Coverage</th></tr>
  <tr><td><code>test_scoring_properties.py</code></td><td>Property-based tests for all scoring calculators</td></tr>
  <tr><td><code>test_board_analyzer.py</code></td><td>Board composition → AI governance signals</td></tr>
  <tr><td><code>test_glassdoor_culture.py</code></td><td>Glassdoor data → culture/talent signals</td></tr>
  <tr><td><code>test_leadership_signals.py</code></td><td>Leadership signal collection</td></tr>
  <tr><td><code>test_evidence_collection.py</code></td><td>SEC evidence ingestion end-to-end</td></tr>
  <tr><td><code>test_api.py</code></td><td>FastAPI endpoint tests</td></tr>
  <tr><td><code>test_models.py</code></td><td>Pydantic model validation</td></tr>
</table>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>13. Key API Reference</h2>
<div class="duration">Duration: 3 minutes</div>

<pre><code># ── Health ────────────────────────────────────────────────────
GET  /health

# ── Companies ─────────────────────────────────────────────────
GET  /api/v1/companies
GET  /api/v1/companies/{id}

# ── Evidence Collection ───────────────────────────────────────
POST /api/v1/evidence/backfill
GET  /api/v1/evidence/stats
GET  /api/v1/companies/{id}/evidence

# ── SEC Documents ─────────────────────────────────────────────
GET  /api/v1/documents?ticker=JPM&amp;limit=10
GET  /api/v1/documents/{id}/chunks

# ── External Signals ──────────────────────────────────────────
GET  /api/v1/signals?company_id={id}
GET  /api/v1/companies/{id}/signals

# ── Scoring ───────────────────────────────────────────────────
POST /api/v1/scores/{id}/compute
POST /api/v1/scores/by-ticker/{ticker}
PUT  /api/v1/scores/{id}
GET  /api/v1/scores/{id}

# ── Assessments ───────────────────────────────────────────────
GET  /api/v1/assessments
POST /api/v1/assessments</code></pre>

<p>Full interactive documentation: <code>http://localhost:8000/docs</code></p>

<hr>

<!-- ══════════════════════════════════════════════ -->
<h2>14. Congratulations!</h2>

<p>You have successfully:</p>
<ul class="checklist">
  <li>Set up the PE Org-AI-R Platform (FastAPI + Snowflake + Redis + Streamlit + Airflow)</li>
  <li>Collected SEC filings and external operational signals for 10 portfolio companies</li>
  <li>Understood the Org-AI-R scoring formula and its seven dimensions</li>
  <li>Computed V^R, H^R, Synergy, and composite Org-AI-R scores with 95% confidence intervals</li>
  <li>Explored the multi-page Streamlit dashboard</li>
  <li>Orchestrated the pipeline with Apache Airflow</li>
  <li>Run the property-based test suite</li>
</ul>

<h3>Team contributions</h3>
<table>
  <tr><th>Member</th><th>Contribution</th></tr>
  <tr><td>Wei Cheng Tu</td><td>Evidence-to-Dimension Mapper, Rubric-Based Scorer (5 dimensions → scores)</td></tr>
  <tr><td>Nisarg Sheth</td><td>Glassdoor Culture Collector (culture dimension), Board Composition Analyzer (AI governance dimension)</td></tr>
  <tr><td>Yu Tzu Li</td><td>Scoring calculator pipeline, API endpoints, property-based tests</td></tr>
</table>

<h3>Resources</h3>
<ul>
  <li>FastAPI public URL: http://35.93.9.162:8000/docs</li>
  <li>Demo video: https://www.youtube.com/watch?v=ATQqYbEYGnM</li>
  <li>GitHub repository: https://github.com/DAMG7245-spring2026/case_study3</li>
  <li>Snowflake free trial: https://signup.snowflake.com/</li>
</ul>

</body>
</html>
